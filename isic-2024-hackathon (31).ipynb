{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"},{"sourceId":8805041,"sourceType":"datasetVersion","datasetId":5295511},{"sourceId":8805074,"sourceType":"datasetVersion","datasetId":5295526},{"sourceId":8805104,"sourceType":"datasetVersion","datasetId":5295545},{"sourceId":3729,"sourceType":"modelInstanceVersion","modelInstanceId":2656,"modelId":312}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> RUN ONLY ON GPU <span style='color:#F1A424'>|</span></b>","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport re\nimport gc\nimport os\nimport io\nimport copy\nimport timm\nimport h5py\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom io import BytesIO\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision.transforms import ToTensor\nfrom torchvision import datasets, transforms\nfrom torch.optim import Adam, SGD, lr_scheduler\nfrom torch.utils.data import Subset, Dataset, DataLoader, ConcatDataset\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score \nfrom sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import StackingClassifier\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.utils import class_weight\n\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import VotingClassifier\nimport lightgbm as lgb\nimport catboost as cb\nimport xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2024-07-31T09:04:56.276099Z","iopub.execute_input":"2024-07-31T09:04:56.276442Z","iopub.status.idle":"2024-07-31T09:05:17.126358Z","shell.execute_reply.started":"2024-07-31T09:04:56.276415Z","shell.execute_reply":"2024-07-31T09:05:17.125443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building the main dataframe and creating the paths to the metadata","metadata":{}},{"cell_type":"code","source":"# Load data\nkaggle_path = '/kaggle/input/isic-2024-challenge'\ntrain_metadata_path = os.path.join(kaggle_path, 'train-metadata.csv')\ntest_metadata_path = os.path.join(kaggle_path, 'test-metadata.csv')\ntrain_image_path = os.path.join(kaggle_path, 'train-image.hdf5')\ntest_image_path = os.path.join(kaggle_path, 'test-image.hdf5')\n\nisic_2024_metadata_df = pd.read_csv(train_metadata_path)\nisic_2024_metadata_test_df = pd.read_csv(test_metadata_path)\n\nisic_2024_metadata_df","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:12.135735Z","iopub.execute_input":"2024-07-31T08:59:12.136125Z","iopub.status.idle":"2024-07-31T08:59:19.275019Z","shell.execute_reply.started":"2024-07-31T08:59:12.136094Z","shell.execute_reply":"2024-07-31T08:59:19.273994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Real Validation","metadata":{}},{"cell_type":"code","source":"benign_df_temporary    = isic_2024_metadata_df[isic_2024_metadata_df['target'] == 0].reset_index(drop=True)\nmalignant_df_temporary = isic_2024_metadata_df[isic_2024_metadata_df['target'] == 1].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:19.276985Z","iopub.execute_input":"2024-07-31T08:59:19.277461Z","iopub.status.idle":"2024-07-31T08:59:19.509143Z","shell.execute_reply.started":"2024-07-31T08:59:19.277424Z","shell.execute_reply":"2024-07-31T08:59:19.508344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"benign_df_untouchable    = benign_df_temporary.iloc[30_000:].reset_index(drop=True)\nmalignant_df_untouchable = malignant_df_temporary.iloc[100:].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:19.510376Z","iopub.execute_input":"2024-07-31T08:59:19.510690Z","iopub.status.idle":"2024-07-31T08:59:19.570489Z","shell.execute_reply.started":"2024-07-31T08:59:19.510666Z","shell.execute_reply":"2024-07-31T08:59:19.569702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"benign_df_untouchable.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:19.572584Z","iopub.execute_input":"2024-07-31T08:59:19.572879Z","iopub.status.idle":"2024-07-31T08:59:19.594846Z","shell.execute_reply.started":"2024-07-31T08:59:19.572853Z","shell.execute_reply":"2024-07-31T08:59:19.593947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"malignant_df_untouchable.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:19.595940Z","iopub.execute_input":"2024-07-31T08:59:19.596255Z","iopub.status.idle":"2024-07-31T08:59:19.621955Z","shell.execute_reply.started":"2024-07-31T08:59:19.596230Z","shell.execute_reply":"2024-07-31T08:59:19.621126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"untouchable_2024_df = pd.concat([malignant_df_untouchable, benign_df_untouchable]).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:19.623081Z","iopub.execute_input":"2024-07-31T08:59:19.623447Z","iopub.status.idle":"2024-07-31T08:59:20.125859Z","shell.execute_reply.started":"2024-07-31T08:59:19.623383Z","shell.execute_reply":"2024-07-31T08:59:20.125067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"untouchable_2024_df['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:20.127205Z","iopub.execute_input":"2024-07-31T08:59:20.127511Z","iopub.status.idle":"2024-07-31T08:59:20.142590Z","shell.execute_reply.started":"2024-07-31T08:59:20.127486Z","shell.execute_reply":"2024-07-31T08:59:20.141802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"benign_df_trainable    = benign_df_temporary.iloc[:30_000].reset_index(drop=True)\nmalignant_df_trainable = malignant_df_temporary.iloc[:300].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:20.143926Z","iopub.execute_input":"2024-07-31T08:59:20.144265Z","iopub.status.idle":"2024-07-31T08:59:20.158166Z","shell.execute_reply.started":"2024-07-31T08:59:20.144235Z","shell.execute_reply":"2024-07-31T08:59:20.157186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"isic_2024_metadata_df = pd.concat([malignant_df_trainable, benign_df_trainable]).sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:20.159362Z","iopub.execute_input":"2024-07-31T08:59:20.159712Z","iopub.status.idle":"2024-07-31T08:59:20.208107Z","shell.execute_reply.started":"2024-07-31T08:59:20.159689Z","shell.execute_reply":"2024-07-31T08:59:20.207145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"isic_2024_metadata_df['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:20.211990Z","iopub.execute_input":"2024-07-31T08:59:20.212378Z","iopub.status.idle":"2024-07-31T08:59:20.219711Z","shell.execute_reply.started":"2024-07-31T08:59:20.212353Z","shell.execute_reply":"2024-07-31T08:59:20.218777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Back to the old code","metadata":{}},{"cell_type":"code","source":"isic_2024_metadata_df.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:20.220827Z","iopub.execute_input":"2024-07-31T08:59:20.221158Z","iopub.status.idle":"2024-07-31T08:59:20.487344Z","shell.execute_reply.started":"2024-07-31T08:59:20.221123Z","shell.execute_reply":"2024-07-31T08:59:20.486385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Device initialization","metadata":{}},{"cell_type":"code","source":"# Get cpu, gpu or mps device for training.\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\nprint(f\"Using {device} device\")","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:20.488434Z","iopub.execute_input":"2024-07-31T08:59:20.488711Z","iopub.status.idle":"2024-07-31T08:59:20.516892Z","shell.execute_reply.started":"2024-07-31T08:59:20.488686Z","shell.execute_reply":"2024-07-31T08:59:20.515925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>1 <span style='color:#F1A424'>|</span> 2024 METADATA Pre-processing <span style='color:#F1A424'>|</span></b>","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering functions\n","metadata":{}},{"cell_type":"code","source":"# Extracting features and labels from the DataFrame\nfeatures_cat = [\"sex\", \"tbp_tile_type\", \"tbp_lv_location\", \"tbp_lv_location_simple\", \"anatom_site_general\",'patient_id']\n\nfeatures_num = [\n    'age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', \n    'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', \n    'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', \n    'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB',\n    'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM',\n    'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color',\n    'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL',\n    'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle',\n    'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z',\n]\nuseless_features = ['lesion_id','attribution', 'copyright_license', 'image_type','iddx_full','iddx_1','iddx_2','iddx_3','iddx_4','iddx_5','mel_mitotic_index']\nforbiden_features = ['mel_thick_mm','tbp_lv_dnn_lesion_confidence']\ntarget = ['target']","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:20.518029Z","iopub.execute_input":"2024-07-31T08:59:20.518325Z","iopub.status.idle":"2024-07-31T08:59:20.528506Z","shell.execute_reply.started":"2024-07-31T08:59:20.518301Z","shell.execute_reply":"2024-07-31T08:59:20.527607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_engineering(df):\n    # New features to try...\n    df[\"lesion_size_ratio\"] = df[\"tbp_lv_minorAxisMM\"] / df[\"clin_size_long_diam_mm\"]\n    df[\"lesion_shape_index\"] = df[\"tbp_lv_areaMM2\"] / (df[\"tbp_lv_perimeterMM\"] ** 2)\n    df[\"hue_contrast\"] = (df[\"tbp_lv_H\"] - df[\"tbp_lv_Hext\"]).abs()\n    df[\"luminance_contrast\"] = (df[\"tbp_lv_L\"] - df[\"tbp_lv_Lext\"]).abs()\n    df[\"lesion_color_difference\"] = np.sqrt(df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2)\n    df[\"border_complexity\"] = df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_symm_2axis\"]\n    df[\"color_uniformity\"] = df[\"tbp_lv_color_std_mean\"] / df[\"tbp_lv_radial_color_std_max\"]\n    df[\"3d_position_distance\"] = np.sqrt(df[\"tbp_lv_x\"] ** 2 + df[\"tbp_lv_y\"] ** 2 + df[\"tbp_lv_z\"] ** 2) \n    df[\"perimeter_to_area_ratio\"] = df[\"tbp_lv_perimeterMM\"] / df[\"tbp_lv_areaMM2\"]\n    df[\"lesion_visibility_score\"] = df[\"tbp_lv_deltaLBnorm\"] + df[\"tbp_lv_norm_color\"]\n    df[\"symmetry_border_consistency\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_norm_border\"]\n    df[\"color_consistency\"] = df[\"tbp_lv_stdL\"] / df[\"tbp_lv_Lext\"]\n    df[\"size_age_interaction\"] = df[\"clin_size_long_diam_mm\"] * df[\"age_approx\"]\n    df[\"hue_color_std_interaction\"] = df[\"tbp_lv_H\"] * df[\"tbp_lv_color_std_mean\"]\n    df[\"lesion_severity_index\"] = (df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_eccentricity\"]) / 3\n    df[\"shape_complexity_index\"] = df[\"border_complexity\"] + df[\"lesion_shape_index\"]\n    df[\"color_contrast_index\"] = df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"] + df[\"tbp_lv_deltaLBnorm\"]\n    df[\"log_lesion_area\"] = np.log(df[\"tbp_lv_areaMM2\"] + 1)\n    df[\"normalized_lesion_size\"] = df[\"clin_size_long_diam_mm\"] / df[\"age_approx\"]\n    df[\"mean_hue_difference\"] = (df[\"tbp_lv_H\"] + df[\"tbp_lv_Hext\"]) / 2\n    df[\"std_dev_contrast\"] = np.sqrt((df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2) / 3)\n    df[\"color_shape_composite_index\"] = (df[\"tbp_lv_color_std_mean\"] + df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_symm_2axis\"]) / 3\n    df[\"3d_lesion_orientation\"] = np.arctan2(df[\"tbp_lv_y\"], df[\"tbp_lv_x\"])\n    df[\"overall_color_difference\"] = (df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"]) / 3\n    df[\"symmetry_perimeter_interaction\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_perimeterMM\"]\n    df[\"comprehensive_lesion_index\"] = (df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_eccentricity\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_symm_2axis\"]) / 4\n\n    new_num_cols = [\n        \"lesion_size_ratio\", \"lesion_shape_index\", \"hue_contrast\",\n        \"luminance_contrast\", \"lesion_color_difference\", \"border_complexity\",\n        \"color_uniformity\", \"3d_position_distance\", \"perimeter_to_area_ratio\",\n        \"lesion_visibility_score\", \"symmetry_border_consistency\", \"color_consistency\",\n\n        \"size_age_interaction\", \"hue_color_std_interaction\", \"lesion_severity_index\", \n        \"shape_complexity_index\", \"color_contrast_index\", \"log_lesion_area\",\n        \"normalized_lesion_size\", \"mean_hue_difference\", \"std_dev_contrast\",\n        \"color_shape_composite_index\", \"3d_lesion_orientation\", \"overall_color_difference\",\n        \"symmetry_perimeter_interaction\", \"comprehensive_lesion_index\",\n    ]\n    return df, new_num_cols","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:20.530173Z","iopub.execute_input":"2024-07-31T08:59:20.530721Z","iopub.status.idle":"2024-07-31T08:59:20.546729Z","shell.execute_reply.started":"2024-07-31T08:59:20.530687Z","shell.execute_reply":"2024-07-31T08:59:20.545872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature engineering and encoding 2024","metadata":{}},{"cell_type":"code","source":"isic_2024_metadata_engineered, new_num_cols = feature_engineering(isic_2024_metadata_df.copy())\n\nnum_cols = features_num + new_num_cols\ntrain_cols = num_cols + features_cat ","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:20.547890Z","iopub.execute_input":"2024-07-31T08:59:20.548267Z","iopub.status.idle":"2024-07-31T08:59:20.585726Z","shell.execute_reply.started":"2024-07-31T08:59:20.548236Z","shell.execute_reply":"2024-07-31T08:59:20.585059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n\ncategory_encoder = OrdinalEncoder(\n    categories='auto', # The encoder will automatically determine the categories for each feature.\n    dtype=int, # ouput them as integers\n    handle_unknown='use_encoded_value', # The encoder will use a specified integer value for these unknown categories.\n    unknown_value=-2, # which is -2 for unknown values\n    encoded_missing_value=-1, # and -1 for encoded missing value\n)\n\nX_cat = category_encoder.fit_transform(isic_2024_metadata_engineered[features_cat])\nfor c, cat_col in enumerate(features_cat):\n    isic_2024_metadata_engineered[cat_col] = X_cat[:, c]\n\nisic_2024_metadata_engineered = isic_2024_metadata_engineered.replace([np.inf, -np.inf], np.nan).fillna(0)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:20.586713Z","iopub.execute_input":"2024-07-31T08:59:20.586974Z","iopub.status.idle":"2024-07-31T08:59:20.804672Z","shell.execute_reply.started":"2024-07-31T08:59:20.586951Z","shell.execute_reply":"2024-07-31T08:59:20.803842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop useless features\nisic_2024_metadata_engineered = isic_2024_metadata_engineered[train_cols + ['target','isic_id']]\nisic_2024_metadata_engineered","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:20.805778Z","iopub.execute_input":"2024-07-31T08:59:20.806055Z","iopub.status.idle":"2024-07-31T08:59:20.843287Z","shell.execute_reply.started":"2024-07-31T08:59:20.806033Z","shell.execute_reply":"2024-07-31T08:59:20.842334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Displaying feature correlation","metadata":{}},{"cell_type":"code","source":"corr_data = isic_2024_metadata_engineered[num_cols + features_cat + ['target']].copy()\ndataplot = sns.heatmap(corr_data.corr(), cmap=\"YlGnBu\", annot=False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:20.844678Z","iopub.execute_input":"2024-07-31T08:59:20.844982Z","iopub.status.idle":"2024-07-31T08:59:21.755176Z","shell.execute_reply.started":"2024-07-31T08:59:20.844955Z","shell.execute_reply":"2024-07-31T08:59:21.754372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlations = isic_2024_metadata_engineered[train_cols+['target']].corr().loc[:, 'target'].abs()\nsorted_correlations = correlations.sort_values(ascending=False)\nprint(sorted_correlations.apply(lambda x: f\"{x*100:.2f}%\"))\ntop_correlated_features = sorted_correlations.index.tolist()[1:6]\nprint(\"Most correlated features to the target :\", top_correlated_features)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:21.756168Z","iopub.execute_input":"2024-07-31T08:59:21.756452Z","iopub.status.idle":"2024-07-31T08:59:22.081555Z","shell.execute_reply.started":"2024-07-31T08:59:21.756422Z","shell.execute_reply":"2024-07-31T08:59:22.080603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>2 <span style='color:#F1A424'>|</span> 2024 IMAGE Preprocessing <span style='color:#F1A424'>|</span></b>\n","metadata":{}},{"cell_type":"markdown","source":"## ISIC 2024 Image Loader Class","metadata":{}},{"cell_type":"code","source":"class ImageLoaderWithMetadata(Dataset):\n    def __init__(self, df, file_hdf, transform=None, subset=None, has_target=True):\n        self.fp_hdf = h5py.File(file_hdf, mode=\"r\")\n        self.transform = transform\n        self.has_target = has_target\n        \n        if subset is not None and subset[1]-subset[0] < len(df):\n            self.df = df.iloc[subset[0]:subset[1]].reset_index(drop=True)\n        else:\n            self.df = df\n        \n        self.isic_ids = self.df['isic_id'].values\n        \n        if self.has_target:\n            self.targets = self.df['target'].values\n        \n    def __len__(self):\n        return len(self.isic_ids)\n    \n    def __getitem__(self, index):\n        isic_id = self.isic_ids[index]\n        image = Image.open(BytesIO(self.fp_hdf[isic_id][()]))\n        \n        if self.transform:\n            image = np.array(image)\n            transformed = self.transform(image=image)\n            image = transformed['image']\n            image = image / 255 \n                \n        if self.has_target:\n            target = self.targets[index]\n            return (image, target)\n        else:\n            return image\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:22.082675Z","iopub.execute_input":"2024-07-31T08:59:22.082928Z","iopub.status.idle":"2024-07-31T08:59:22.092519Z","shell.execute_reply.started":"2024-07-31T08:59:22.082907Z","shell.execute_reply":"2024-07-31T08:59:22.091631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Image displaying functions","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom numpy import random    \n\ndef display_random_grid_images(dataset, n=3, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    \n    # Get n*n unique random indices\n    indices = random.choice(len(dataset), n*n, replace=False)\n    \n    # Create a grid of subplots\n    fig, axs = plt.subplots(n, n, figsize=(10, 10))\n    \n    # Flatten the axs array to iterate easily\n    axs = axs.flatten()\n    \n    # Select random images and display them\n    for i, idx in enumerate(indices):\n        img, target = dataset[idx]\n        isic_id = dataset.isic_ids[idx]\n        if isinstance(img, torch.Tensor):\n            img = img.permute(1, 2, 0).numpy()\n\n        axs[i].imshow(img)\n        axs[i].axis('off')\n        target_name = 'Benign' if target==0 else 'Malignant'\n        axs[i].set_title(f'ID: {isic_id} \\n Target: {target_name}')\n    \n    # Hide any extra subplots\n    for j in range(n*n, len(axs)):\n        axs[j].axis('off')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:22.093761Z","iopub.execute_input":"2024-07-31T08:59:22.094128Z","iopub.status.idle":"2024-07-31T08:59:22.106652Z","shell.execute_reply.started":"2024-07-31T08:59:22.094098Z","shell.execute_reply":"2024-07-31T08:59:22.105724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation functions\n","metadata":{}},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimage_size = (137, 137) # (height, width)\n\n\ntrain_transform_and_augment = A.Compose([\n    A.SmallestMaxSize(max_size=137),  # Resize to a smaller dimension while keeping aspect ratio\n    A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.01, rotate_limit=30, p=1),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),   \n    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.5, p=0.8),\n    A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.3),  # Slight R, G and B shift\n    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=0.1),  # Slight changes to hue, saturation, and value (brightness)\n    \n    A.Resize(image_size[0], image_size[1]),  # Resize to the target size\n    ToTensorV2(),  # Convert the image to a PyTorch tensor\n])\n\ntrain_transform_no_augment = A.Compose([\n    A.Resize(image_size[0], image_size[1]),  # Resize to the target size\n    ToTensorV2(),  # Convert the image to a PyTorch tensor\n])","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:22.110759Z","iopub.execute_input":"2024-07-31T08:59:22.111050Z","iopub.status.idle":"2024-07-31T08:59:22.194372Z","shell.execute_reply.started":"2024-07-31T08:59:22.111028Z","shell.execute_reply":"2024-07-31T08:59:22.193463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image Loading and Augmentation","metadata":{}},{"cell_type":"code","source":"isic_2024_benign_df = isic_2024_metadata_engineered[isic_2024_metadata_engineered['target'] == 0].reset_index(drop=True)\nisic_2024_malignant_df = isic_2024_metadata_engineered[isic_2024_metadata_engineered['target'] == 1].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:22.195556Z","iopub.execute_input":"2024-07-31T08:59:22.196846Z","iopub.status.idle":"2024-07-31T08:59:22.213068Z","shell.execute_reply.started":"2024-07-31T08:59:22.196813Z","shell.execute_reply":"2024-07-31T08:59:22.212305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the sample models\nsubset_models = [(0, 10_000), (10_000, 20_000), (20_000, 30_000)]\n\n# Function to create benign datasets for a given model\ndef create_benign_datasets(subset):\n    benign_no_augment = ImageLoaderWithMetadata(df=isic_2024_benign_df, file_hdf=train_image_path, transform=train_transform_no_augment, subset=subset)\n    benign_augment = ImageLoaderWithMetadata(df=isic_2024_benign_df, file_hdf=train_image_path, transform=train_transform_and_augment, subset=subset)\n    return benign_no_augment, benign_augment\n\n# Function to create malignant datasets with augmentations\ndef create_malignant_datasets():\n    malignant_base = ImageLoaderWithMetadata(df=isic_2024_malignant_df, file_hdf=train_image_path, transform=train_transform_no_augment)\n    malignant_aug_1 = ImageLoaderWithMetadata(df=isic_2024_malignant_df, file_hdf=train_image_path, transform=train_transform_and_augment)\n    malignant_aug_2 = ImageLoaderWithMetadata(df=isic_2024_malignant_df, file_hdf=train_image_path, transform=train_transform_and_augment)\n    malignant_aug_3 = ImageLoaderWithMetadata(df=isic_2024_malignant_df, file_hdf=train_image_path, transform=train_transform_and_augment)\n    return [malignant_base, malignant_aug_1, malignant_aug_2, malignant_aug_3]\n\n# Create benign datasets for each model\nbenign_dataset_model_1, _ = create_benign_datasets(subset_models[0])\nbenign_dataset_model_2, _ = create_benign_datasets(subset_models[1])\nbenign_dataset_model_3, _ = create_benign_datasets(subset_models[2])\n\n# Create malignant datasets\nmalignant_dataset_model_1 = create_malignant_datasets()\nmalignant_dataset_model_2 = create_malignant_datasets()\nmalignant_dataset_model_3 = create_malignant_datasets()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:22.214063Z","iopub.execute_input":"2024-07-31T08:59:22.214307Z","iopub.status.idle":"2024-07-31T08:59:22.248452Z","shell.execute_reply.started":"2024-07-31T08:59:22.214285Z","shell.execute_reply":"2024-07-31T08:59:22.247656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>3 <span style='color:#F1A424'>|</span> 2018-19-20 IMAGE Pre-processing <span style='color:#F1A424'>|</span></b>","metadata":{}},{"cell_type":"markdown","source":"## Loading Paths and DataFrames","metadata":{}},{"cell_type":"code","source":"# Load data 2020\ntrain_2020_metadata_path = '/kaggle/input/isic-2020-jpg-256x256-resized/train-metadata.csv'\ntrain_2019_metadata_path = '/kaggle/input/isic-2019-jpg-256x256-resized/train-metadata.csv'\ntrain_2018_metadata_path = '/kaggle/input/isic-2018-jpg-256x256-resized/train-metadata.csv'\n\ntrain_2020_jpg_image_path = '/kaggle/input/isic-2020-jpg-256x256-resized/train-image/image'\ntrain_2019_jpg_image_path = '/kaggle/input/isic-2019-jpg-256x256-resized/train-image/image'\ntrain_2018_jpg_image_path = '/kaggle/input/isic-2018-jpg-256x256-resized/train-image/image'\n\nisic_2020_metadata_df = pd.read_csv(train_2020_metadata_path).drop(columns=['Unnamed: 0','patient_id'],inplace=False)\nisic_2019_metadata_df = pd.read_csv(train_2019_metadata_path).drop(columns=['Unnamed: 0','patient_id'],inplace=False)\nisic_2018_metadata_df = pd.read_csv(train_2018_metadata_path).drop(columns=['Unnamed: 0','patient_id'],inplace=False)\n\nisic_2020_metadata_df","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:22.249602Z","iopub.execute_input":"2024-07-31T08:59:22.249937Z","iopub.status.idle":"2024-07-31T08:59:22.360732Z","shell.execute_reply.started":"2024-07-31T08:59:22.249905Z","shell.execute_reply":"2024-07-31T08:59:22.359821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing corrupted ISIC-2018 Images","metadata":{}},{"cell_type":"code","source":"tqdm.pandas()\n\nimport tensorflow as tf\n\ncheck_path = lambda p: tf.io.gfile.exists(p)\nisic_2018_metadata_df['target'] = isic_2018_metadata_df['target'].astype(int) # 0.0 -> 0\nisic_2018_metadata_df.dropna(axis=0, inplace=True)\n\nprint(\"\\nChecking 2018 image files ...\")\nisic_2018_metadata_df['exists'] = (train_2018_jpg_image_path+'/'+isic_2018_metadata_df['isic_id']+'.jpg').progress_apply(check_path)\nisic_2018_metadata_df['exists'].value_counts()\nisic_2018_metadata_df = isic_2018_metadata_df[isic_2018_metadata_df['exists'] == True].drop(columns=['exists'],inplace=False).reset_index()\nisic_2018_metadata_df.drop(columns=['index'],inplace=True)\nisic_2018_metadata_df","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:22.362921Z","iopub.execute_input":"2024-07-31T08:59:22.363213Z","iopub.status.idle":"2024-07-31T08:59:34.416336Z","shell.execute_reply.started":"2024-07-31T08:59:22.363188Z","shell.execute_reply":"2024-07-31T08:59:34.415354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Keep as much of benign as of malignant for 2020-2019-2018","metadata":{}},{"cell_type":"code","source":"def keep_balanced(df):\n    malignant_df = df[df['target']==1]\n    benign_df = df[df['target']==0].iloc[:len(malignant_df.index)]\n    return pd.concat([malignant_df,benign_df]).reset_index(drop=True)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:34.418031Z","iopub.execute_input":"2024-07-31T08:59:34.418350Z","iopub.status.idle":"2024-07-31T08:59:34.424553Z","shell.execute_reply.started":"2024-07-31T08:59:34.418326Z","shell.execute_reply":"2024-07-31T08:59:34.423684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"isic_2020_metadata_df = keep_balanced(isic_2020_metadata_df)\nisic_2019_metadata_df = keep_balanced(isic_2019_metadata_df)\nisic_2018_metadata_df = keep_balanced(isic_2018_metadata_df)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:34.425613Z","iopub.execute_input":"2024-07-31T08:59:34.425879Z","iopub.status.idle":"2024-07-31T08:59:34.446204Z","shell.execute_reply.started":"2024-07-31T08:59:34.425857Z","shell.execute_reply":"2024-07-31T08:59:34.445196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(isic_2020_metadata_df['target'].value_counts())\nprint(isic_2019_metadata_df['target'].value_counts())\nprint(isic_2018_metadata_df['target'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:34.448533Z","iopub.execute_input":"2024-07-31T08:59:34.448936Z","iopub.status.idle":"2024-07-31T08:59:34.456143Z","shell.execute_reply.started":"2024-07-31T08:59:34.448907Z","shell.execute_reply":"2024-07-31T08:59:34.455233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction proper to 2018-2019-2020","metadata":{}},{"cell_type":"code","source":"train_transform_and_augment_for_jpgs = A.Compose([\n    A.SmallestMaxSize(max_size=137),  # Resize to a smaller dimension while keeping aspect ratio\n    \n    ## Match 2024 Look \n\n    A.RandomBrightnessContrast(brightness_limit=(-0.1, 0), contrast_limit=(-0.3, 0), p=0.8),  # Reduce brightness and contrast\n    A.GaussianBlur(blur_limit=(1, 3), p=1),\n    A.RandomGamma(gamma_limit=(80, 120), p=1),\n    A.GaussNoise(var_limit=(10.0, 20.0), p=1),\n    A.MotionBlur(blur_limit=(3, 7), p=0.5),  # Introduce motion blur to simulate hand-held camera movement\n    A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=0.5),  # Introduce optical distortion\n\n    # Real Augmentation\n    \n    A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.01, rotate_limit=30, p=1),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),    \n    A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.3),  # Slight R, G and B shift\n    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=0.1),  # Slight changes to hue, saturation, and value (brightness)\n    \n    \n    A.Resize(image_size[0], image_size[1]),  # Resize to the target size\n    ToTensorV2(),  # Convert the image to a PyTorch tensor\n])","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:34.457231Z","iopub.execute_input":"2024-07-31T08:59:34.457547Z","iopub.status.idle":"2024-07-31T08:59:34.469927Z","shell.execute_reply.started":"2024-07-31T08:59:34.457524Z","shell.execute_reply":"2024-07-31T08:59:34.468956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ISIC 2018-19-20 Image Loader Class","metadata":{}},{"cell_type":"code","source":"class ISICDataset_jpgs(Dataset):\n    def __init__(self, df, image_path, has_target=False, transform=None, reference_columns=None, missing_value=-1):\n        self.df = df\n        self.label = df['target']\n        self.isic_ids = df['isic_id']\n        self.transform = transform\n        self.img_path = os.path.join(image_path)\n        self.has_target = has_target\n        if self.has_target:\n            self.targets = self.df['target'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        isic_id = self.isic_ids[index]\n        img_path = os.path.join(self.img_path, isic_id + '.jpg')  \n        image = Image.open(img_path)\n        \n        if self.transform:\n            image = np.array(image)\n            transformed = self.transform(image=image)  # Apply transformation\n            image = transformed['image']\n            image = image / 255 \n                \n        if self.has_target:\n            target = self.targets[index]\n            return (image, target) \n        else:\n            return image\n        \n    def get_labels(self):\n        return self.label","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:34.471019Z","iopub.execute_input":"2024-07-31T08:59:34.471344Z","iopub.status.idle":"2024-07-31T08:59:34.480925Z","shell.execute_reply.started":"2024-07-31T08:59:34.471313Z","shell.execute_reply":"2024-07-31T08:59:34.480100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_2020_dataset_aug = ISICDataset_jpgs(df = isic_2020_metadata_df, image_path = train_2020_jpg_image_path, has_target = True, transform=train_transform_and_augment_for_jpgs)\nimage_2019_dataset_aug = ISICDataset_jpgs(df = isic_2019_metadata_df, image_path = train_2019_jpg_image_path, has_target = True, transform=train_transform_and_augment_for_jpgs)\nimage_2018_dataset_aug = ISICDataset_jpgs(df = isic_2018_metadata_df, image_path = train_2018_jpg_image_path, has_target = True, transform=train_transform_and_augment_for_jpgs)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:34.482018Z","iopub.execute_input":"2024-07-31T08:59:34.482281Z","iopub.status.idle":"2024-07-31T08:59:34.495015Z","shell.execute_reply.started":"2024-07-31T08:59:34.482258Z","shell.execute_reply":"2024-07-31T08:59:34.494099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image Displaying And Comparision","metadata":{}},{"cell_type":"code","source":"seed = 42\n\ndisplay_random_grid_images(malignant_dataset_model_1[1],      n=3, seed=seed)\ndisplay_random_grid_images(image_2020_dataset_aug, n=3, seed=seed)\ndisplay_random_grid_images(image_2019_dataset_aug, n=3, seed=seed)\ndisplay_random_grid_images(image_2018_dataset_aug, n=3, seed=seed)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:34.496067Z","iopub.execute_input":"2024-07-31T08:59:34.496320Z","iopub.status.idle":"2024-07-31T08:59:42.337331Z","shell.execute_reply.started":"2024-07-31T08:59:34.496299Z","shell.execute_reply":"2024-07-31T08:59:42.336459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Number of 2020 samples in the augmented dataset : {len(image_2020_dataset_aug.isic_ids)}')\nprint(f'Number of 2019 samples in the augmented dataset : {len(image_2019_dataset_aug.isic_ids)}')\nprint(f'Number of 2018 samples in the augmented dataset : {len(image_2018_dataset_aug.isic_ids)}')","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:42.338342Z","iopub.execute_input":"2024-07-31T08:59:42.338635Z","iopub.status.idle":"2024-07-31T08:59:42.343877Z","shell.execute_reply.started":"2024-07-31T08:59:42.338609Z","shell.execute_reply":"2024-07-31T08:59:42.343028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DataFrames Concatenation","metadata":{}},{"cell_type":"markdown","source":"I created a class that in addition to concatenate the datasets, it creates a target attribute to the object","metadata":{}},{"cell_type":"code","source":"class ConcatDatasetWithMetadataAndTarget(ConcatDataset):\n    def __init__(self, datasets):\n        super().__init__(datasets) \n        self.targets = np.concatenate([dataset.targets for dataset in datasets])\n    \n    def __getitem__(self, idx):\n        image, target = super().__getitem__(idx)\n        \n        target = self.targets[idx]\n        return image, target","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:42.347467Z","iopub.execute_input":"2024-07-31T08:59:42.347753Z","iopub.status.idle":"2024-07-31T08:59:42.358716Z","shell.execute_reply.started":"2024-07-31T08:59:42.347730Z","shell.execute_reply":"2024-07-31T08:59:42.357746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_concat_previous_years_dataset = ConcatDatasetWithMetadataAndTarget([image_2020_dataset_aug, image_2019_dataset_aug, image_2018_dataset_aug])","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:42.359905Z","iopub.execute_input":"2024-07-31T08:59:42.360198Z","iopub.status.idle":"2024-07-31T08:59:42.370579Z","shell.execute_reply.started":"2024-07-31T08:59:42.360175Z","shell.execute_reply":"2024-07-31T08:59:42.369616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Trying without previous years\n# train_2024_model_1 = ConcatDatasetWithMetadataAndTarget([*benign_dataset_model_1, *malignant_dataset_model_1, train_concat_previous_years_dataset])\n# train_2024_model_2 = ConcatDatasetWithMetadataAndTarget([*benign_dataset_model_2, *malignant_dataset_model_2, train_concat_previous_years_dataset])\n# train_2024_model_3 = ConcatDatasetWithMetadataAndTarget([*benign_dataset_model_3, *malignant_dataset_model_3, train_concat_previous_years_dataset])\n\ntrain_previous_years_1 = ConcatDatasetWithMetadataAndTarget([train_concat_previous_years_dataset])\ntrain_previous_years_2 = ConcatDatasetWithMetadataAndTarget([train_concat_previous_years_dataset])\ntrain_previous_years_3 = ConcatDatasetWithMetadataAndTarget([train_concat_previous_years_dataset])\n\ntrain_2024_model_1 = ConcatDatasetWithMetadataAndTarget([benign_dataset_model_1, *malignant_dataset_model_1])\ntrain_2024_model_2 = ConcatDatasetWithMetadataAndTarget([benign_dataset_model_2, *malignant_dataset_model_2])\ntrain_2024_model_3 = ConcatDatasetWithMetadataAndTarget([benign_dataset_model_3, *malignant_dataset_model_3])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:42.371755Z","iopub.execute_input":"2024-07-31T08:59:42.372016Z","iopub.status.idle":"2024-07-31T08:59:42.381561Z","shell.execute_reply.started":"2024-07-31T08:59:42.371994Z","shell.execute_reply":"2024-07-31T08:59:42.380733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_final_dataset = ConcatDatasetWithMetadataAndTarget([train_concat_2024_dataset, train_concat_previous_years_dataset])\n#train_final_dataset = train_concat_2024_dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:42.382644Z","iopub.execute_input":"2024-07-31T08:59:42.382940Z","iopub.status.idle":"2024-07-31T08:59:42.393240Z","shell.execute_reply.started":"2024-07-31T08:59:42.382906Z","shell.execute_reply":"2024-07-31T08:59:42.392289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total number of images :', len(train_2024_model_1))","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:42.394792Z","iopub.execute_input":"2024-07-31T08:59:42.395615Z","iopub.status.idle":"2024-07-31T08:59:42.407434Z","shell.execute_reply.started":"2024-07-31T08:59:42.395592Z","shell.execute_reply":"2024-07-31T08:59:42.406585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b> 5 <span style='color:#F1A424'>|</span> CNN Training <span style='color:#F1A424'>|</span></b>","metadata":{}},{"cell_type":"markdown","source":"## Creating Training Functions and Classes","metadata":{}},{"cell_type":"markdown","source":"### Definition of Generalized Mean Pooling (GeM)\n\n$\\text{GeM}(x) = \\left( \\frac{1}{H \\times W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (\\max(x(i,j), \\epsilon))^p \\right)^{\\frac{1}{p}}$\n\nWith $x$ with shape $(N, C, H, W)$\n\nWe recognize $ \\text{GeM}(x) = ∥max(x,ϵ)∥_p$\n\nThis process allows the GeM layer to interpolate between average pooling (when $p=1$) and max pooling (as $p \\to \\infty$). For $p > 1$, it emphasizes larger values in the feature map more than average pooling does.","metadata":{}},{"cell_type":"code","source":"class GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM, self).__init__()\n        self.p = nn.Parameter(torch.ones(1)*p)\n        self.eps = eps # epsilon to avoid division by 0\n\n    def gem(self, x, p=3, eps=1e-6):\n        clamped_x = x.clamp(min=eps) # Operation quite straight forward looking at the formula\n        pow_clamped_x = clamped_x.pow(p)\n        average_pool = F.avg_pool2d(pow_clamped_x, (x.size(-2), x.size(-1)))\n        gem = average_pool.pow(1/p)\n        return gem\n    \n    def forward(self, x):\n        return self.gem(x, p = self.p, eps = self.eps) # Apply the gem function seen before\n        \n    def __repr__(self):\n        # hard function just to output : GeM(p=3.0000, eps=1e-06)\n        # with obviously the current p and eps values\n        return  self.__class__.__name__ +\\\n                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n                ', ' + 'eps=' + str(self.eps) + ')'","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:42.408554Z","iopub.execute_input":"2024-07-31T08:59:42.408882Z","iopub.status.idle":"2024-07-31T08:59:42.418032Z","shell.execute_reply.started":"2024-07-31T08:59:42.408853Z","shell.execute_reply":"2024-07-31T08:59:42.417146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ISICModel(nn.Module):\n    def __init__(self, model_name, num_metadata_features, num_classes=1, checkpoint_path=None):\n        super(ISICModel, self).__init__()\n        self.model = timm.create_model(model_name, checkpoint_path=checkpoint_path)\n        in_features = self.model.classifier.in_features\n        self.in_features = in_features\n        self.model.classifier = nn.Identity()\n        self.model.global_pool = nn.Identity()\n        \n        self.pooling = GeM()\n        self.agressive_dropout  = nn.Dropout(0.5)\n        self.soft_dropout       = nn.Dropout(0.2)\n        self.linear             = nn.Linear(in_features, num_classes)\n        self.sigmoid            = nn.Sigmoid()\n\n    def forward(self, images):\n        features = self.model(images)\n        pooled_features = self.pooling(features).flatten(1)\n        output = self.sigmoid(self.linear(pooled_features))\n        return output\n\nmodel_name = 'efficientnet_b0'\nnum_metadata_features = 65\ncheckpoint_path = '/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b0/1/tf_efficientnet_b0_aa-827b6e33.pth'\n\ncnn_model_1 = ISICModel(model_name, num_metadata_features, checkpoint_path=checkpoint_path)\ncnn_model_2 = ISICModel(model_name, num_metadata_features, checkpoint_path=checkpoint_path)\ncnn_model_3 = ISICModel(model_name, num_metadata_features, checkpoint_path=checkpoint_path)\ncnn_model_1 = cnn_model_1.to(device)\ncnn_model_2 = cnn_model_2.to(device)\ncnn_model_3 = cnn_model_3.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:42.419158Z","iopub.execute_input":"2024-07-31T08:59:42.419441Z","iopub.status.idle":"2024-07-31T08:59:43.207065Z","shell.execute_reply.started":"2024-07-31T08:59:42.419419Z","shell.execute_reply":"2024-07-31T08:59:43.206065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Weighted Loss function","metadata":{}},{"cell_type":"code","source":"def criterion(outputs, targets):\n    weights = 1 + 4 * targets\n    outputs = outputs.view(-1, 1)  # Ensure outputs have shape (N, 1)\n    loss = nn.BCELoss()(outputs, targets) * weights.sum()\n    return loss  # Binary Loss function","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:43.208274Z","iopub.execute_input":"2024-07-31T08:59:43.208575Z","iopub.status.idle":"2024-07-31T08:59:43.213825Z","shell.execute_reply.started":"2024-07-31T08:59:43.208550Z","shell.execute_reply":"2024-07-31T08:59:43.212904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building the Training functions","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    running_auroc  = 0.0\n    running_corrects = 0.0\n    running_recall = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, (images, targets) in bar:\n        images = images.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float).unsqueeze(1)\n        \n        batch_size = images.size(0)\n        \n        outputs = model(images).squeeze()\n        \n        loss = criterion(outputs, targets)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        outputs_cpu = outputs.detach().cpu().numpy()\n        targets_cpu = targets.cpu().numpy()\n        \n        try:\n            auroc = roc_auc_score(targets_cpu, outputs_cpu, max_fpr=0.8).item()\n        except Exception as e:\n            auroc = 0.0\n        \n        preds = (outputs_cpu > 0.5).astype(int)\n        accuracy  = accuracy_score(targets_cpu, preds)\n        recall    = recall_score(targets_cpu, preds)\n        \n        running_loss += (loss.item() * batch_size)\n        running_auroc  += (auroc * batch_size)\n        running_corrects += (accuracy * batch_size)\n        running_recall += (recall * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss   = running_loss     / dataset_size\n        epoch_auroc  = running_auroc    / dataset_size\n        epoch_acc    = running_corrects / dataset_size\n        epoch_recall = running_recall   / dataset_size\n        \n        if scheduler is not None:\n            scheduler.step(epoch_loss)\n        \n        bar.set_postfix(Epoch=epoch, Train_Acc=f\"{epoch_acc:.2f}%\", Train_Recall=f\"{epoch_recall:.2f}%\", Train_Loss=epoch_loss, Train_Auroc=epoch_auroc,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    \n    return epoch_loss, epoch_auroc, epoch_acc, epoch_recall\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:43.215031Z","iopub.execute_input":"2024-07-31T08:59:43.215290Z","iopub.status.idle":"2024-07-31T08:59:43.229616Z","shell.execute_reply.started":"2024-07-31T08:59:43.215268Z","shell.execute_reply":"2024-07-31T08:59:43.228790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Disabling gradient computation and saving memory\n@torch.inference_mode()\ndef valid_one_epoch(model, dataloader, device, epoch, optimizer):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    running_auroc = 0.0\n    running_corrects = 0.0\n    running_recall = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, (images, targets) in bar: \n        images = images.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float).unsqueeze(1)\n        \n        batch_size = images.size(0)\n\n        outputs = model(images).squeeze()\n        \n        loss = criterion(outputs, targets)\n        \n        outputs_cpu = outputs.detach().cpu().numpy()\n        targets_cpu = targets.cpu().numpy()\n        \n        try:\n            auroc = roc_auc_score(targets_cpu, outputs_cpu, max_fpr=0.8).item()\n        except Exception as e:\n            auroc = 0.0\n        \n        preds = (outputs_cpu > 0.5).astype(int)\n        \n        accuracy = accuracy_score(targets_cpu, preds)\n        recall    = recall_score(targets_cpu, preds)\n        \n        running_loss += (loss.item() * batch_size)\n        running_auroc  += (auroc * batch_size)\n        running_corrects += (accuracy * batch_size)\n        running_recall += (recall * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss   = running_loss     / dataset_size\n        epoch_auroc  = running_auroc    / dataset_size\n        epoch_acc    = running_corrects / dataset_size\n        epoch_recall = running_recall   / dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Valid_Acc=f\"{epoch_acc:.2f}%\", Valid_Recall=f\"{epoch_recall:.2f}%\", Valid_Loss=epoch_loss, Valid_Auroc=epoch_auroc,\n                        LR=optimizer.param_groups[0]['lr'])   \n    \n    gc.collect()\n    \n    return epoch_loss, epoch_auroc, epoch_acc, epoch_recall","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:43.230876Z","iopub.execute_input":"2024-07-31T08:59:43.231384Z","iopub.status.idle":"2024-07-31T08:59:43.245302Z","shell.execute_reply.started":"2024-07-31T08:59:43.231354Z","shell.execute_reply":"2024-07-31T08:59:43.244522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\ndef run_training(model, optimizer, scheduler, device, num_epochs, train_loader, valid_loader):\n    # Confirm that it is running on GPU\n    if torch.cuda.is_available():\n        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n    \n    start = time.time()\n    \n    # Deep copies the initial model weights to save the best model later\n    best_model_wts = copy.deepcopy(model.state_dict())\n    \n    # Initializes the best AUROC to 0.\n    best_epoch_auroc = 0\n    \n    # Initializa dictionary (Better way than history = {} because history['Train Loss'].append(train_epoch_loss) is possible even is 'Train Loss' isn't a key yet)\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        \n        # Train for one epoch\n        train_epoch_loss, train_epoch_auroc, train_epoch_acc, train_epoch_recall = train_one_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader, \n                                           device=device, epoch=epoch)\n        # Valid for one epoch\n        val_epoch_loss, val_epoch_auroc, val_epoch_acc, val_epoch_recall = valid_one_epoch(model, valid_loader, optimizer = optimizer, device=device, \n                                         epoch=epoch)\n        \n        # Save the loss and score in the history dict\n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(val_epoch_loss)\n        history['Train AUROC'].append(train_epoch_auroc)\n        history['Valid AUROC'].append(train_epoch_acc)\n        history['Train Acc'].append(train_epoch_auroc)\n        history['Valid Acc'].append(val_epoch_acc)\n        history['Train Recall'].append(train_epoch_recall)\n        history['Valid Recall'].append(val_epoch_recall)\n        history['lr'].append(optimizer.param_groups[0]['lr'])\n        \n        # Save the model if it's getting better results\n        # if best_epoch_auroc <= val_epoch_auroc:\n        if True : # just got rid of this criterium\n            print(f\"{best_epoch_auroc} Validation AUROC Improved ({best_epoch_auroc} ---> {val_epoch_auroc})\")\n            \n            # Updates best auroc\n            best_epoch_auroc = val_epoch_auroc\n            \n            # Deepcopy the weights\n            best_model_wts = copy.deepcopy(model.state_dict())\n            \n            # Saves the weights in the working directory\n            PATH = \"/kaggle/working/AUROC{:.4f}_Loss{:.4f}_epoch{:.0f}.bin\".format(val_epoch_auroc, val_epoch_loss, epoch)\n            torch.save(model.state_dict(), PATH)\n            \n            print(f\"Model Saved\")\n            \n        print()\n    \n    end = time.time()\n    \n    # Display the training time\n    time_elapsed = end - start # in seconds\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600 , (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best AUROC: {:.4f}\".format(best_epoch_auroc))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:43.246690Z","iopub.execute_input":"2024-07-31T08:59:43.246962Z","iopub.status.idle":"2024-07-31T08:59:43.261543Z","shell.execute_reply.started":"2024-07-31T08:59:43.246940Z","shell.execute_reply":"2024-07-31T08:59:43.260728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cosine Annealing Learning Rate Formula\n\nThe learning rate $\\eta_t$ at time $t$ is computed as:\n\n$\n\\eta_t = \\eta_{\\min} + \\frac{1}{2} (\\eta_{\\max} - \\eta_{\\min}) \\left(1 + \\cos\\left(\\frac{T_{cur}}{T_{max}} \\pi\\right)\\right)\n$\n\nWhere:\n- $\\eta_t$ is the learning rate at epoch $t$.\n- $\\eta_{\\min}$ is the minimum learning rate (`eta_min`).\n- $\\eta_{\\max}$ is the initial (maximum) learning rate (starting learning rate of the optimizer).\n- $T_{cur}$ is the current number of iterations (epochs) completed.\n- $T_{max}$ is the maximum number of iterations (epochs) for one cycle.\n","metadata":{}},{"cell_type":"code","source":"def init_opti_and_scheduler(model):\n    optimizer = Adam(model.parameters(), \n                    lr=1e-4, \n                    weight_decay=1e-6)\n\n    scheduler = lr_scheduler.CosineAnnealingLR(\n        optimizer,\n        T_max=1400, # 2 times the number of steps \n        eta_min=1e-6) # Minimum learning Rate Value\n    \n    return optimizer, scheduler\n\n# optimizer_temp, scheduler_temp = init_opti_and_scheduler(cnn_temp_model)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:43.262718Z","iopub.execute_input":"2024-07-31T08:59:43.263150Z","iopub.status.idle":"2024-07-31T08:59:43.275280Z","shell.execute_reply.started":"2024-07-31T08:59:43.263121Z","shell.execute_reply":"2024-07-31T08:59:43.274449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Splitting","metadata":{}},{"cell_type":"code","source":"batch_size = 64 \n\ntrain_loader_model_1 = DataLoader(train_2024_model_1, batch_size=batch_size, shuffle=True , num_workers=4)\ntrain_loader_model_2 = DataLoader(train_2024_model_2, batch_size=batch_size, shuffle=True , num_workers=4)\ntrain_loader_model_3 = DataLoader(train_2024_model_3, batch_size=batch_size, shuffle=True , num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:43.276445Z","iopub.execute_input":"2024-07-31T08:59:43.276730Z","iopub.status.idle":"2024-07-31T08:59:43.285545Z","shell.execute_reply.started":"2024-07-31T08:59:43.276707Z","shell.execute_reply":"2024-07-31T08:59:43.284723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_engineered, _ = feature_engineering(untouchable_2024_df.copy())\ncnn_validation_dataset  = ImageLoaderWithMetadata(df=valid_engineered, file_hdf=train_image_path, transform=train_transform_no_augment, has_target=True, subset=(0, 10_000))\nvalidation_dataset  = ImageLoaderWithMetadata(df=valid_engineered, file_hdf=train_image_path, transform=train_transform_no_augment, has_target=True)\ncnn_validation_loader = DataLoader(cnn_validation_dataset, batch_size=batch_size, shuffle=True , num_workers=4)\nvalidation_loader   = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True , num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:43.286687Z","iopub.execute_input":"2024-07-31T08:59:43.287159Z","iopub.status.idle":"2024-07-31T08:59:43.477336Z","shell.execute_reply.started":"2024-07-31T08:59:43.287135Z","shell.execute_reply":"2024-07-31T08:59:43.476465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Launch Training","metadata":{}},{"cell_type":"code","source":"optimizer_1, scheduler_1 = init_opti_and_scheduler(cnn_model_1)\noptimizer_2, scheduler_2 = init_opti_and_scheduler(cnn_model_2)\noptimizer_3, scheduler_3 = init_opti_and_scheduler(cnn_model_3)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:43.478437Z","iopub.execute_input":"2024-07-31T08:59:43.478708Z","iopub.status.idle":"2024-07-31T08:59:43.489221Z","shell.execute_reply.started":"2024-07-31T08:59:43.478685Z","shell.execute_reply":"2024-07-31T08:59:43.488256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training Model 1 ...')\ncnn_model_model_1, history_1 = run_training(cnn_model_1, \n                              optimizer_1, \n                              scheduler_1,\n                              train_loader = train_loader_model_1, \n                              valid_loader = cnn_validation_loader,\n                              device=device,\n                              num_epochs = 5)\n\nprint('Training Model 2 ...')\ncnn_model_model_2, history_2 = run_training(cnn_model_2, \n                              optimizer_2, \n                              scheduler_2,\n                              train_loader = train_loader_model_2, \n                              valid_loader = cnn_validation_loader,\n                              device=device,\n                              num_epochs = 5)\n\nprint('Training Model 3 ...')\ncnn_model_model_3, history_3 = run_training(cnn_model_3, \n                              optimizer_3, \n                              scheduler_3,\n                              train_loader = train_loader_model_3, \n                              valid_loader = cnn_validation_loader,\n                              device=device,\n                              num_epochs = 5)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:43.490997Z","iopub.execute_input":"2024-07-31T08:59:43.491295Z","iopub.status.idle":"2024-07-31T08:59:47.510596Z","shell.execute_reply.started":"2024-07-31T08:59:43.491269Z","shell.execute_reply":"2024-07-31T08:59:47.506466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNN Training Results Visualization","metadata":{}},{"cell_type":"code","source":"history = pd.DataFrame.from_dict(history_3)\nhistory.to_csv('history.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.512058Z","iopub.status.idle":"2024-07-31T08:59:47.512553Z","shell.execute_reply.started":"2024-07-31T08:59:47.512294Z","shell.execute_reply":"2024-07-31T08:59:47.512314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(history.shape[0]), history[\"Train Acc\"].values, label=\"Train Acc\")\nplt.plot(range(history.shape[0]), history[\"Valid Acc\"].values, label=\"Valid Acc\")\nplt.xlabel(\"epochs\")\nplt.ylabel(\"Accuracy\")\nplt.xlim(0, history.shape[0])\nplt.ylim(0, 1)\nplt.grid()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.514178Z","iopub.status.idle":"2024-07-31T08:59:47.514644Z","shell.execute_reply.started":"2024-07-31T08:59:47.514390Z","shell.execute_reply":"2024-07-31T08:59:47.514424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(history.shape[0]), history[\"Train Loss\"].values, label=\"Train Loss\")\nplt.plot(range(history.shape[0]), history[\"Valid Loss\"].values, label=\"Valid Loss\")\nplt.xlabel(\"epochs\")\nplt.ylabel(\"Loss\")\nplt.xlim(0, history.shape[0])\nplt.ylim(0, np.max(history[\"Train Loss\"].values))\nplt.grid()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.516237Z","iopub.status.idle":"2024-07-31T08:59:47.516585Z","shell.execute_reply.started":"2024-07-31T08:59:47.516391Z","shell.execute_reply":"2024-07-31T08:59:47.516423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot( range(history.shape[0]), history[\"Train Recall\"].values, label=\"Train Recall\")\nplt.plot( range(history.shape[0]), history[\"Valid Recall\"].values, label=\"Valid Recall\")\nplt.xlabel(\"epochs\")\nplt.ylabel(\"Recall\")\nplt.xlim(0, history.shape[0])\nplt.ylim(0, 1.0)\nplt.grid()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.517834Z","iopub.status.idle":"2024-07-31T08:59:47.518155Z","shell.execute_reply.started":"2024-07-31T08:59:47.517997Z","shell.execute_reply":"2024-07-31T08:59:47.518011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot( range(history.shape[0]), history[\"Train AUROC\"].values, label=\"Train AUROC\")\nplt.plot( range(history.shape[0]), history[\"Valid AUROC\"].values, label=\"Valid AUROC\")\nplt.xlabel(\"epochs\")\nplt.ylabel(\"AUROC\")\nplt.xlim(0, history.shape[0])\nplt.ylim(0, 1.0)\nplt.grid()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.520149Z","iopub.status.idle":"2024-07-31T08:59:47.520520Z","shell.execute_reply.started":"2024-07-31T08:59:47.520318Z","shell.execute_reply":"2024-07-31T08:59:47.520332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot( range(history.shape[0]), history[\"lr\"].values, label=\"lr\")\nplt.xlabel(\"epochs\")\nplt.ylabel(\"lr\")\nplt.grid()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.522253Z","iopub.status.idle":"2024-07-31T08:59:47.522609Z","shell.execute_reply.started":"2024-07-31T08:59:47.522439Z","shell.execute_reply":"2024-07-31T08:59:47.522453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Displaying Mistaken predictions","metadata":{}},{"cell_type":"code","source":"def get_mistaken_predictions(model, dataloader, device):\n    model.eval()\n    mistaken_images = {'false_positives': [], 'false_negatives': []}\n    \n    with torch.inference_mode():\n        for images, targets in dataloader:\n            images = images.to(device, dtype=torch.float)\n            targets = targets.to(device, dtype=torch.float)\n            outputs = model(images).squeeze()\n            preds = (outputs > 0.5).int()\n            \n            for i in range(len(preds)):\n                if preds[i] != targets[i]:\n                    if preds[i] == 1 and targets[i] == 0:\n                        mistaken_images['false_positives'].append((images[i].cpu(), targets[i].cpu()))\n                    elif preds[i] == 0 and targets[i] == 1:\n                        mistaken_images['false_negatives'].append((images[i].cpu(), targets[i].cpu()))\n    \n    return mistaken_images\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.523797Z","iopub.status.idle":"2024-07-31T08:59:47.524120Z","shell.execute_reply.started":"2024-07-31T08:59:47.523961Z","shell.execute_reply":"2024-07-31T08:59:47.523975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_mistaken_images(mistaken_images, category='false_positives', n=5):\n    images_to_display = mistaken_images[category][:n]\n    fig, axs = plt.subplots(1, n, figsize=(20, 5))\n    for i, (img, target) in enumerate(images_to_display):\n        if isinstance(img, torch.Tensor):\n            img = img.permute(1, 2, 0).numpy()\n        axs[i].imshow(img)\n        axs[i].axis('off')\n        if category == 'false_positives':\n            axs[i].set_title(f'Target: Benign \\n Prediction : Malignant', fontsize=16)\n        else : \n            axs[i].set_title(f'Target: Malignant \\n Prediction : Benign', fontsize=16)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.525365Z","iopub.status.idle":"2024-07-31T08:59:47.525802Z","shell.execute_reply.started":"2024-07-31T08:59:47.525582Z","shell.execute_reply":"2024-07-31T08:59:47.525599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mistaken_images = get_mistaken_predictions(cnn_model_1, cnn_validation_loader, device)\nprint(\"Displaying false positives:\")\ndisplay_mistaken_images(mistaken_images, 'false_positives', n=5)\nprint(\"Displaying false negatives:\")\ndisplay_mistaken_images(mistaken_images, 'false_negatives', n=5)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.527091Z","iopub.status.idle":"2024-07-31T08:59:47.528469Z","shell.execute_reply.started":"2024-07-31T08:59:47.528228Z","shell.execute_reply":"2024-07-31T08:59:47.528252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>6 <span style='color:#F1A424'>|</span> Metadata Gradient Boosting <span style='color:#F1A424'>|</span></b>","metadata":{}},{"cell_type":"markdown","source":"## AUC-ROC","metadata":{}},{"cell_type":"code","source":"def comp_score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, min_tpr: float=0.80):\n    v_gt = abs(np.asarray(solution.values)-1)\n    v_pred = np.array([1.0 - x for x in submission.values])\n    max_fpr = abs(1-min_tpr)\n    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n    return partial_auc","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.529726Z","iopub.status.idle":"2024-07-31T08:59:47.530088Z","shell.execute_reply.started":"2024-07-31T08:59:47.529923Z","shell.execute_reply":"2024-07-31T08:59:47.529937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNN Predictions","metadata":{}},{"cell_type":"code","source":"@torch.inference_mode()\ndef GradBoost_predictions(model, dataloader, device):\n    model.eval()\n    torch.cuda.empty_cache()\n    predictions_list = []\n    \n    for step, (images, target) in tqdm(enumerate(dataloader), desc='Loading predictions...', total = len(dataloader)):  \n        images = images.to(device, dtype=torch.float)\n        outputs = model(images).squeeze()\n        if outputs.dim() == 1:\n            predictions_list.extend(outputs.tolist())\n        else:\n            predictions_list.extend(outputs.squeeze().tolist())\n    return predictions_list","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.531736Z","iopub.status.idle":"2024-07-31T08:59:47.532085Z","shell.execute_reply.started":"2024-07-31T08:59:47.531922Z","shell.execute_reply":"2024-07-31T08:59:47.531936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"isic_2024_metadata_df = pd.read_csv(train_metadata_path)\nGrad_boost_full_df = untouchable_2024_df\n# Grad_boost_full_df = isic_2024_metadata_df\nGrad_boost_full_df_engineered, _ = feature_engineering(Grad_boost_full_df.copy())","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.534341Z","iopub.status.idle":"2024-07-31T08:59:47.534802Z","shell.execute_reply.started":"2024-07-31T08:59:47.534571Z","shell.execute_reply":"2024-07-31T08:59:47.534589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_Grad_boost_dataset = ImageLoaderWithMetadata(df=Grad_boost_full_df_engineered, file_hdf=train_image_path, transform=train_transform_no_augment)\nlen(full_Grad_boost_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.536232Z","iopub.status.idle":"2024-07-31T08:59:47.536716Z","shell.execute_reply.started":"2024-07-31T08:59:47.536466Z","shell.execute_reply":"2024-07-31T08:59:47.536485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Keep Full DataSet ??\nGradBoost_Loader = DataLoader(full_Grad_boost_dataset, shuffle=True, batch_size=batch_size, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.538049Z","iopub.status.idle":"2024-07-31T08:59:47.538496Z","shell.execute_reply.started":"2024-07-31T08:59:47.538253Z","shell.execute_reply":"2024-07-31T08:59:47.538272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_preds_1 = GradBoost_predictions(cnn_model_1, GradBoost_Loader, device)\ncnn_preds_2 = GradBoost_predictions(cnn_model_2, GradBoost_Loader, device)\ncnn_preds_3 = GradBoost_predictions(cnn_model_3, GradBoost_Loader, device)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.540044Z","iopub.status.idle":"2024-07-31T08:59:47.540558Z","shell.execute_reply.started":"2024-07-31T08:59:47.540275Z","shell.execute_reply":"2024-07-31T08:59:47.540296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_predictions_df = pd.DataFrame({\n    'isic_id': full_Grad_boost_dataset.isic_ids,\n    'prediction_score_model_1': cnn_preds_1,\n    'prediction_score_model_2': cnn_preds_2,\n    'prediction_score_model_3': cnn_preds_3,\n})","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.542277Z","iopub.status.idle":"2024-07-31T08:59:47.542742Z","shell.execute_reply.started":"2024-07-31T08:59:47.542507Z","shell.execute_reply":"2024-07-31T08:59:47.542525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge with the original test metadata\n# merged_df = pd.merge(isic_2024_metadata_df, cnn_predictions_df, on='isic_id')\nmerged_df = pd.merge(untouchable_2024_df, cnn_predictions_df, on='isic_id')\n# Display the merged DataFrame\nmerged_df","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.543946Z","iopub.status.idle":"2024-07-31T08:59:47.544371Z","shell.execute_reply.started":"2024-07-31T08:59:47.544149Z","shell.execute_reply":"2024-07-31T08:59:47.544166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.546142Z","iopub.status.idle":"2024-07-31T08:59:47.546616Z","shell.execute_reply.started":"2024-07-31T08:59:47.546352Z","shell.execute_reply":"2024-07-31T08:59:47.546370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, new_num_cols = feature_engineering(merged_df.copy())\nnum_cols = features_num + new_num_cols\ntrain_cols = num_cols + features_cat + ['prediction_score_model_1','prediction_score_model_2','prediction_score_model_3']\n# train_cols = num_cols + features_cat","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.548109Z","iopub.status.idle":"2024-07-31T08:59:47.548560Z","shell.execute_reply.started":"2024-07-31T08:59:47.548314Z","shell.execute_reply":"2024-07-31T08:59:47.548331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def engineer_predictions(df):\n    # Convert DataFrame columns to PyTorch tensors and move them to GPU\n    pred1 = torch.tensor(df['prediction_score_model_1'].values, device=device, dtype=torch.float32)\n    pred2 = torch.tensor(df['prediction_score_model_2'].values, device=device, dtype=torch.float32)\n    pred3 = torch.tensor(df['prediction_score_model_3'].values, device=device, dtype=torch.float32)\n\n    # Calculate ratios using GPU\n    ratio_1_over_2 = pred1 / pred2\n    ratio_2_over_3 = pred2 / pred3\n    ratio_3_over_1 = pred3 / pred1\n\n    # Move the calculated ratios back to CPU and convert to numpy arrays\n    df['ratio_1_over_2'] = ratio_1_over_2.cpu().numpy()\n    df['ratio_2_over_3'] = ratio_2_over_3.cpu().numpy()\n    df['ratio_3_over_1'] = ratio_3_over_1.cpu().numpy()\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.549780Z","iopub.status.idle":"2024-07-31T08:59:47.550209Z","shell.execute_reply.started":"2024-07-31T08:59:47.549985Z","shell.execute_reply":"2024-07-31T08:59:47.550003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n\ncategory_encoder = OrdinalEncoder(\n    categories='auto', # The encoder will automatically determine the categories for each feature.\n    dtype=int, # ouput them as integers\n    handle_unknown='use_encoded_value', # The encoder will use a specified integer value for these unknown categories.\n    unknown_value=-2, # which is -2 for unknown values\n    encoded_missing_value=-1, # and -1 for encoded missing value\n)\n\nX_cat = category_encoder.fit_transform(df_train[features_cat])\nfor c, cat_col in enumerate(features_cat):\n    df_train[cat_col] = X_cat[:, c]\n\ndf_train = df_train.replace([np.inf, -np.inf], np.nan).fillna(0)\n\n# Standard scaling for numerical features\nscaler = StandardScaler()\ndf_train[train_cols] = scaler.fit_transform(df_train[train_cols])\n\n# train_cols += ['ratio_1_over_2','ratio_2_over_3','ratio_3_over_1']\n\n# df_train = engineer_predictions(df_train)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.551470Z","iopub.status.idle":"2024-07-31T08:59:47.551913Z","shell.execute_reply.started":"2024-07-31T08:59:47.551676Z","shell.execute_reply":"2024-07-31T08:59:47.551696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[train_cols].describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.552905Z","iopub.status.idle":"2024-07-31T08:59:47.553232Z","shell.execute_reply.started":"2024-07-31T08:59:47.553068Z","shell.execute_reply":"2024-07-31T08:59:47.553081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[train_cols]","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.555340Z","iopub.status.idle":"2024-07-31T08:59:47.555793Z","shell.execute_reply.started":"2024-07-31T08:59:47.555565Z","shell.execute_reply":"2024-07-31T08:59:47.555583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_malignant = df_train[df_train['target']==1][train_cols]\ndf_train_benign = df_train[df_train['target']==0][train_cols]","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:47.556836Z","iopub.status.idle":"2024-07-31T08:59:47.557253Z","shell.execute_reply.started":"2024-07-31T08:59:47.557037Z","shell.execute_reply":"2024-07-31T08:59:47.557055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"number_of_fold_trainings = 5 # 10*0.5 to prevent overfitting","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gkf = GroupKFold(n_splits=10)\n\ndf_train[\"fold\"] = -1 # Setting temp value\n# gkf.split is generating splits for cross-validation\n# groups=df_train[\"patient_id\"] ensures that the data is split such that the same patient doesn't appear in both training and validation sets.\n#  train_idx, val_idx are two arrays containing the indices for the training and validation sets\nfor idx, (train_idx, val_idx) in enumerate(gkf.split(df_train, df_train[\"target\"], groups=df_train[\"patient_id\"])):\n    df_train.loc[val_idx, \"fold\"] = idx # Assign a fold number from 0 to 9\n    print(f\"Fold {idx}: {len(val_idx)} validation samples\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"# Gamma : Specifies the minimum loss reduction required to make a split.\n# Number of leaves \n# Lambda L2\n# Bagging frequency\n# min child samples ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\ntorch.cuda.empty_cache()\n\nbest_params_xgb = {\n    'objective': 'binary:logistic', # Binary classification\n    'eval_metric': 'logloss', # Loss function\n    'colsample_bytree': 0.8684, \n    'lambda': 20, \n    'learning_rate': 0.6098, \n    'max_depth': 10, \n    'n_estimators': 2727, \n    'reg_alpha': 5.5721, \n    'reg_lambda': 25.9040, \n    'subsample': 1.0,\n    'nthread': 4,\n    'random_state': 42,\n    'tree_method': 'gpu_hist',\n    'verbosity': 0 # Silent mode\n    \n}\n\nxgb_scores = []\nxgb_models = []\n\nfor fold in range(0,number_of_fold_trainings):\n    # If fold = 1\n    _df_train = df_train[df_train[\"fold\"] != fold].reset_index(drop=True) # _df_train contains every fold except 1\n    _df_valid = df_train[df_train[\"fold\"] == fold].reset_index(drop=True) # _df_valid is the first fold\n    model = xgb.XGBClassifier(**best_params_xgb)\n    model.fit(_df_train[train_cols], _df_train[\"target\"]) \n    preds = model.predict_proba(_df_valid[train_cols])[:, 1]\n    score = comp_score(_df_valid[[\"target\"]], pd.DataFrame(preds, columns=[\"prediction\"]), \"\")\n\n    print(f\"fold: {fold} - ROC AUC Score: {score:.5f}\")\n    xgb_scores.append(score)\n    xgb_models.append(model)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_score = np.mean(xgb_scores)\nprint(f\"XGBoost Average ROC AUC Score: {xgb_score:.5f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances = np.mean([model.feature_importances_ for model in xgb_models], axis=0)\ndf_imp = pd.DataFrame({\"feature\": train_cols, \"importance\": importances}).sort_values(\"importance\").reset_index(drop=True)\n\nplt.figure(figsize=(16, 12))\nplt.barh(df_imp[\"feature\"], df_imp[\"importance\"])\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.title(\"Feature Importance\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CAT Model","metadata":{}},{"cell_type":"code","source":"%%time\nimport catboost as cb\ntorch.cuda.empty_cache()\n\ncb_scores = []\ncb_models = []\n\ncb_params = {\n    'objective': 'Logloss',\n    \"random_state\": 42,\n    \"colsample_bylevel\": 0.3,\n    \"iterations\": 400,\n    \"learning_rate\": 0.05,\n    \"max_depth\": 8,\n    \"l2_leaf_reg\": 5,\n    \"scale_pos_weight\": 2,\n    \"verbose\": 0,\n}\n\nfor fold in tqdm(range(1,number_of_fold_trainings+1), desc= 'Training over Folds', total = 5):\n    _df_train = df_train[df_train[\"fold\"] != fold].reset_index(drop=True)\n    _df_valid = df_train[df_train[\"fold\"] == fold].reset_index(drop=True)\n    #model = cb.CatBoostClassifier(**cb_params)\n    model = VotingClassifier([(f\"cb_{i}\", cb.CatBoostClassifier(**cb_params)) for i in range(3)], voting=\"soft\")\n    model.fit(_df_train[train_cols], _df_train[\"target\"])\n    preds = model.predict_proba(_df_valid[train_cols])[:, 1]\n    score = comp_score(_df_valid[[\"target\"]], pd.DataFrame(preds, columns=[\"prediction\"]), \"\")\n    print(f\"fold: {fold} - Partial AUC Score: {score:.5f}\")\n    cb_scores.append(score)\n    cb_models.append(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cb_score = np.mean(cb_scores)\nprint(f\"CatBoost Score: {cb_score:.5f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nimportances = np.mean([model.get_feature_importance() for model in cb_models], axis=0)\ndf_imp = pd.DataFrame({\"feature\": train_cols, \"importance\": importances}).sort_values(\"importance\").reset_index(drop=True)\n\nplt.figure(figsize=(16, 12))\nplt.barh(df_imp[\"feature\"], df_imp[\"importance\"])\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.title(\"Feature Importances from CatBoost Model\")\nplt.show()\n'''\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM","metadata":{}},{"cell_type":"code","source":"%%time\nimport lightgbm as lgb\n\nlgb_params = {\n    \"objective\": \"binary\",\n    \"verbosity\": -1,\n    \"boosting_type\": \"gbdt\",\n    \"n_estimators\": 200,\n    'learning_rate': 0.05,    \n    'lambda_l1': 0.0004, \n    'lambda_l2': 8.7652, \n    'num_leaves': 136, \n    'feature_fraction': 0.5392, \n    'bagging_fraction': 0.9577, \n    'bagging_freq': 6,\n    'min_child_samples': 60,\n    \"device\": \"gpu\"\n}\n\n\nlgb_scores = []\nlgb_models = []\n\nfor fold in tqdm(range(1,number_of_fold_trainings+1), desc= 'Training over Folds', total = 5):\n    _df_train = df_train[df_train[\"fold\"] != fold].reset_index(drop=True)\n    _df_valid = df_train[df_train[\"fold\"] == fold].reset_index(drop=True)\n    model = lgb.LGBMClassifier(**lgb_params)\n    model = VotingClassifier([(f\"lgb_{i}\", lgb.LGBMClassifier(random_state=i, **lgb_params)) for i in range(7)], voting=\"soft\")\n    # model.fit(_df_train[train_cols], _df_train[\"target\"])\n    preds = model.predict_proba(_df_valid[train_cols])[:, 1]\n    score = comp_score(_df_valid[[\"target\"]], pd.DataFrame(preds, columns=[\"prediction\"]), \"\")\n    print(f\"fold: {fold} - Partial AUC Score: {score:.5f}\")\n    lgb_scores.append(score)\n    lgb_models.append(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_score = np.mean(lgb_scores)\nprint(f\"LGBM Score: {lgbm_score:.5f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nimportances = np.mean([model.feature_importances_ for model in lgb_models], 0)\ndf_imp = pd.DataFrame({\"feature\": model.feature_name_, \"importance\": importances}).sort_values(\"importance\").reset_index(drop=True)\n\nplt.figure(figsize=(16, 12))\nplt.barh(df_imp[\"feature\"], df_imp[\"importance\"])\nplt.show()\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>7 <span style='color:#F1A424'>|</span> Weighting predictions <span style='color:#F1A424'>|</span></b>","metadata":{}},{"cell_type":"markdown","source":"Out-of-Fold (OOF) predictions are used in ensemble methods like stacking to create unbiased training data for the meta-model. Here's why OOF predictions are important and beneficial:","metadata":{}},{"cell_type":"code","source":"@torch.inference_mode()\ndef evaluation_predictions(model, dataloader, device, fold):\n    model.eval()\n    torch.cuda.empty_cache()\n    predictions_list = []\n    \n    for step, (images, metadata, target) in tqdm(enumerate(dataloader), total = len(dataloader), desc=f'Predicting fold [{fold+1}/5]'):   \n        images = images.to(device, dtype=torch.float)\n        metadata = metadata.to(device, dtype=torch.float)\n        outputs = model(images, metadata).squeeze()\n        if outputs.dim() == 1:\n            predictions_list.extend(outputs.tolist())\n        else:\n            predictions_list.extend(outputs.squeeze().tolist())\n    return predictions_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_preds = []\nvalid_preds = []\nvalid_targets = []\n\nfor fold in range(1,number_of_fold_trainings+1):\n    _df_train = df_train[df_train[\"fold\"] != fold].reset_index(drop=True)\n    _df_valid = df_train[df_train[\"fold\"] == fold].reset_index(drop=True)\n    \n    # Predictions for LGBM\n    lgb_model = lgb_models[fold-1]\n    lgb_valid_preds = lgb_model.predict_proba(_df_valid[train_cols])[:, 1]\n    \n    # Predictions for CatBoost\n    cb_model = cb_models[fold-1]\n    cb_valid_preds = cb_model.predict_proba(_df_valid[train_cols])[:, 1]\n    \n    # Predictions for XGBoost\n    xgb_model = xgb_models[fold-1]\n    xgb_valid_preds = xgb_model.predict_proba(_df_valid[train_cols])[:, 1]\n    \n    # Aggregate predictions\n    valid_preds.append(np.column_stack((lgb_valid_preds, cb_valid_preds, xgb_valid_preds)))\n    valid_targets.append(_df_valid[\"target\"].values)\n\nvalid_preds = np.vstack(valid_preds)\nvalid_targets = np.hstack(valid_targets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom itertools import product\n\n# Define a range of weights to test\nweights = np.arange(0, 1.1, 0.1)\nbest_score = 0\nbest_weights = (0.6, 0.4, 0) # (LGBM, CAT, XGB)\nw1, w2, w3 = best_weights\n\n\nfor w1, w2, w3 in tqdm(product(weights, repeat=3), desc ='Testing every combination', total = len(list(product(weights, repeat=3)))):\n    if w1 + w2 + w3 == 1:  # Ensure the weights sum to 1\n        meta_valid_preds = w1 * valid_preds[:, 0] + w2 * valid_preds[:, 1] + w3 * valid_preds[:, 2]\n        score = roc_auc_score(valid_targets, meta_valid_preds)\n        if score > best_score  and w3 < 0.2 and w1 < 0.6 and 0.0 not in (w1, w2, w3):\n            best_score = score\n            best_weights = (w1, w2, w3)\n\nprint(f\"Best weights (LGBM, CAT, XGB): {best_weights}, Best ROC AUC Score: {score:.5f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>8 <span style='color:#F1A424'>|</span> Submission <span style='color:#F1A424'>|</span></b>","metadata":{}},{"cell_type":"markdown","source":"## Test MetaData loading and encoding","metadata":{}},{"cell_type":"code","source":"metadata_and_features_test_df = pd.read_csv(test_metadata_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test ImageLoader","metadata":{}},{"cell_type":"code","source":"@torch.inference_mode()\ndef test_predictions(model, dataloader, device):\n    model.eval()\n    torch.cuda.empty_cache()\n    predictions_list = []\n    \n    for step, (images) in enumerate(dataloader):   \n        images = images.to(device, dtype=torch.float)\n        outputs = model(images).squeeze()\n        if step == 0:\n            print(outputs.tolist()[:5])\n        if outputs.dim() == 1:\n            predictions_list.extend(outputs.tolist())\n        else:\n            predictions_list.extend(outputs.squeeze().tolist())\n    return predictions_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = ImageLoaderWithMetadata(metadata_and_features_test_df, test_image_path, transform=train_transform_no_augment, has_target=False)\ntest_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size, num_workers=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting CNN predictions","metadata":{}},{"cell_type":"code","source":"cnn_test_preds_1 = test_predictions(\n    model=cnn_model_1, \n    dataloader=test_loader,\n    device=device\n)\n\ncnn_test_preds_2 = test_predictions(\n    model=cnn_model_2, \n    dataloader=test_loader,\n    device=device\n)\n\ncnn_test_preds_3 = test_predictions(\n    model=cnn_model_3, \n    dataloader=test_loader,\n    device=device\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_test_df = pd.DataFrame({\n    'isic_id': test_dataset.isic_ids,\n    'prediction_score_model_1': cnn_test_preds_1,\n    'prediction_score_model_2': cnn_test_preds_2,\n    'prediction_score_model_3': cnn_test_preds_3,\n})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_test_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge with the original test metadata\nmerged_test_df = pd.merge(metadata_and_features_test_df, cnn_test_df, on='isic_id')\n# Display the merged DataFrame\nmerged_test_df = merged_test_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting Grandient Boosting Predictions","metadata":{}},{"cell_type":"code","source":"train_cols = num_cols + features_cat + ['prediction_score_model_1','prediction_score_model_2','prediction_score_model_3']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test, new_num_cols = feature_engineering(merged_test_df.copy())\n\ncategory_encoder = OrdinalEncoder(\n    categories='auto', # The encoder will automatically determine the categories for each feature.\n    dtype=int, # ouput them as integers\n    handle_unknown='use_encoded_value', # The encoder will use a specified integer value for these unknown categories.\n    unknown_value=-2, # which is -2 for unknown values\n    encoded_missing_value=-1, # and -1 for encoded missing value\n)\n\nX_cat = category_encoder.fit_transform(df_test[features_cat])\nfor c, cat_col in enumerate(features_cat):\n    df_test[cat_col] = X_cat[:, c]\n    \ndf_test = df_test.replace([np.inf, -np.inf], np.nan).fillna(0)\n\ndf_test[train_cols] = scaler.fit_transform(df_test[train_cols])\n\n# train_cols = num_cols + features_cat + ['ratio_1_over_2','ratio_2_over_3','ratio_3_over_1']\n\n# df_test = engineer_predictions(df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test[train_cols]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_preds = np.mean([model.predict_proba(df_test[train_cols])[:, 1] for model in lgb_models], 0)\ncb_preds = np.mean([model.predict_proba(df_test[train_cols])[:, 1] for model in cb_models], 0)\nxgb_preds = np.mean([model.predict_proba(df_test[train_cols])[:, 1] for model in xgb_models], 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1, w2, w3 = best_weights\n# final_preds = (w1 * lgb_preds + w2 * cb_preds + w3 * xgb_preds)*0.66 + 0.34 * cnn_test_preds\n\nfinal_preds = (w1 * lgb_preds + w2 * cb_preds + w3 * xgb_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submitting","metadata":{}},{"cell_type":"code","source":"df_sub = pd.read_csv(\"/kaggle/input/isic-2024-challenge/sample_submission.csv\")\ndf_sub[\"target\"] = final_preds\ndf_sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}